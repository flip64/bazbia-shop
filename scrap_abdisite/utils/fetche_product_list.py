import requests
from bs4 import BeautifulSoup
import json
import time
import os
from datetime import datetime, timedelta
import re

# -------------------------------
# ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ
# -------------------------------

def slugify(text):
    """ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ slug Ø¨Ø¯ÙˆÙ† Django"""
    text = text.lower()
    text = re.sub(r"[^\w\s-]", "", text)  # Ø­Ø°Ù Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ ØºÛŒØ±Ù…Ø¬Ø§Ø²
    text = re.sub(r"[\s_-]+", "-", text)  # ÙØ§ØµÙ„Ù‡ Ùˆ _ Ùˆ - Ø¨Ù‡ -
    text = re.sub(r"^-+|-+$", "", text)   # Ø­Ø°Ù - Ø¯Ø± Ø§Ø¨ØªØ¯Ø§ Ùˆ Ø§Ù†ØªÙ‡Ø§
    return text

# -------------------------------
# Ù…Ø³ÛŒØ±Ù‡Ø§ Ùˆ ØªÙ†Ø¸ÛŒÙ…Ø§Øª
# -------------------------------

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))  # scrap_abdisite/
OUTPUT_DIR = os.path.join(BASE_DIR, "data", "raw")
os.makedirs(OUTPUT_DIR, exist_ok=True)

headers = {"User-Agent": "Mozilla/5.0"}

list_cat = [
    "game-and-fun", "home-goods", "beauty-and-health",
    "digital-goods", "car-accessories", "sports-and-travel",
    "baby-and-infant", "fashion-and-clothing"
]

# -------------------------------
# ØªÙˆØ§Ø¨Ø¹ Ù…Ø¯ÛŒØ±ÛŒØª ÙØ§ÛŒÙ„ Ùˆ Ø²Ù…Ø§Ù†
# -------------------------------

def get_last_file():
    files = [f for f in os.listdir(OUTPUT_DIR) if f.startswith("raw_") and f.endswith(".json")]
    if not files:
        return None
    files.sort(key=lambda f: os.path.getmtime(os.path.join(OUTPUT_DIR, f)), reverse=True)
    return os.path.join(OUTPUT_DIR, files[0])

def is_need_scrap():
    last_file = get_last_file()
    if not last_file:
        return True  # Ù‡ÛŒÚ† ÙØ§ÛŒÙ„ÛŒ Ù†ÛŒØ³Øª â†’ Ø¨Ø§ÛŒØ¯ Ø§Ø³Ú©Ø±Ù¾ Ú©Ù†ÛŒÙ…

    last_time = datetime.fromtimestamp(os.path.getmtime(last_file))
    if datetime.now() - last_time > timedelta(hours=12):
        return True
    else:
        print(f"â³ Ø¢Ø®Ø±ÛŒÙ† Ú¯Ø²Ø§Ø±Ø´ ({os.path.basename(last_file)}) Ú©Ù…ØªØ± Ø§Ø² Û±Û² Ø³Ø§Ø¹Øª Ù¾ÛŒØ´ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡. Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ Ø§Ø³Ú©Ø±Ù¾ Ù†ÛŒØ³Øª.")
        return False

# -------------------------------
# ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø§Ø³Ú©Ø±Ù¾
# -------------------------------

def fetche_products_list():
    """Ø§Ø³Ú©Ø±Ù¾ Ù…Ø­ØµÙˆÙ„Ø§Øª Ù¾Ø®Ø´ Ø¹Ø¨Ø¯ÛŒ ÛŒØ§ Ø®ÙˆØ§Ù†Ø¯Ù† Ø¢Ø®Ø±ÛŒÙ† ÙØ§ÛŒÙ„ Ø°Ø®ÛŒØ±Ù‡â€ŒØ´Ø¯Ù‡"""
    products = []
    total_count = 0

    if not is_need_scrap():
        last_file = get_last_file()
        if last_file:
            with open(last_file, "r", encoding="utf-8") as f:
                products = json.load(f)
            return products, last_file
        else:
            return [], None

    session = requests.Session()
    session.headers.update(headers)

    for cat in list_cat:
        page = 1
        while True:
            url = f"https://pakhshabdi.com/product-category/{cat}/page/{page}/"
            print("=" * 60)
            print(f"ğŸ“¦ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ: {cat}")
            print(f"ğŸ“„ Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ ØµÙØ­Ù‡ {page} Ø§Ø² Ø¯Ø³ØªÙ‡ {cat}...")
            print(f"ğŸ”— URL: {url}")

            try:
                response = session.get(url, timeout=10)
                if response.status_code != 200:
                    print("âŒ ØµÙØ­Ù‡ ÛŒØ§ÙØª Ù†Ø´Ø¯ ÛŒØ§ Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯.")
                    break
            except Exception as e:
                print(f"ğŸš« Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª ØµÙØ­Ù‡: {e}")
                break

            soup = BeautifulSoup(response.content, "html.parser")
            items = soup.find_all("li", class_="col-md-3 col-6 mini-product-con type-product")

            if not items:
                print("ğŸ“­ Ù‡ÛŒÚ† Ù…Ø­ØµÙˆÙ„ÛŒ Ø¯Ø± Ø§ÛŒÙ† ØµÙØ­Ù‡ ÛŒØ§ÙØª Ù†Ø´Ø¯. Ù¾Ø§ÛŒØ§Ù† Ø§ÛŒÙ† Ø¯Ø³ØªÙ‡.")
                break

            print(f"âœ… {len(items)} Ù…Ø­ØµÙˆÙ„ Ø¯Ø± ØµÙØ­Ù‡ {page} ÛŒØ§ÙØª Ø´Ø¯.")
            no_price_in_row = 0

            for item in items:
                name_tag = item.find("h2", class_="product-title")
                name_link_tag = name_tag.find("a") if name_tag else None
                if not name_link_tag:
                    continue

                name = name_link_tag.text.strip()
                link = name_link_tag.get("href", "Ù„ÛŒÙ†Ú© ÛŒØ§ÙØª Ù†Ø´Ø¯")
                slug = slugify(name)

                price_tag = item.find("span", class_="woocommerce-Price-amount")
                if not price_tag:
                    no_price_in_row += 1
                    if no_price_in_row >= 3:
                        break
                    continue
                else:
                    no_price_in_row = 0

                price_text = price_tag.text.strip()
                price_clean = re.sub(r"\D", "", price_text)
                try:
                    price = int(price_clean)
                except:
                    continue

                img_div = item.find("div", class_="img-con")
                img_tag = img_div.find("img") if img_div else None
                main_image = (
                    img_tag.get("src")
                    or img_tag.get("data-src")
                    or img_tag.get("data-lazy-src")
                    if img_tag else ""
                )

                desc_tag = item.find("div", class_="product-excerpt")
                description = desc_tag.text.strip() if desc_tag else ""

                products.append({
                    "id": None,
                    "name": name,
                    "slug": slug,
                    "description": description,
                    "checked_description": False,
                    "price": price,
                    "quantity": None,
                    "category": cat,
                    "tags": [],
                    "checked_tags": False,
                    "specifications": [],
                    "checked_specs": False,
                    "features": [],
                    "images": [main_image] if main_image else [],
                    "checked_images": False,
                    "videos": [],
                    "product_link": link,
                    "variants": [],
                    "has_variants": False
                })

                total_count += 1
                print(f"â• '{name}' Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯. Ù…Ø¬Ù…ÙˆØ¹ Ù…Ø­ØµÙˆÙ„Ø§Øª: {total_count}")

            if no_price_in_row >= 3:
                break

            page += 1
            time.sleep(1)

    # Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    filename = f"raw_{timestamp}_{total_count}.json"
    file_path = os.path.join(OUTPUT_DIR, filename)

    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(products, f, ensure_ascii=False, indent=2)

    print("=" * 60)
    print(f"âœ… ÙØ§ÛŒÙ„ Â«{file_path}Â» Ø¨Ø§ {total_count} Ù…Ø­ØµÙˆÙ„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")

    return products, file_path

# -------------------------------
# Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø³ØªÙ‚ÛŒÙ… ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ
# -------------------------------

if __name__ == "__main__":
    products, file_path = fetche_products_list()
    print(f"\nğŸ“ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ù…Ø­ØµÙˆÙ„Ø§Øª: {len(products)}")
    print(f"ğŸ—‚ï¸ Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ Ø°Ø®ÛŒØ±Ù‡â€ŒØ´Ø¯Ù‡: {file_path}")
