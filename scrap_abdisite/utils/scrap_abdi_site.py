# scrap_abdisite/utils/scrap_abdi_site.py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import json
from datetime import datetime
from scrap_abdisite.utils.abdi_fetcher import extract_specifications, extract_tags, extract_product_images
from scrap_abdisite.utils.abdi_fetcher import extract_quantity
from time import sleep
import logging

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))  # Ù…ÛŒØ´Ù‡ scrap_abdisite/
RAW_FOLDER = os.path.join(BASE_DIR, "data/raw")
EDITED_FOLDER = os.path.join(BASE_DIR, "data/edited")
os.makedirs(EDITED_FOLDER, exist_ok=True)

# ÙØ§ÛŒÙ„ Ù‚ÙÙ„ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø§Ø¬Ø±Ø§ÛŒ Ù‡Ù…Ø²Ù…Ø§Ù†
LOCK_FILE = os.path.join(EDITED_FOLDER, ".running.lock")

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯
LOG_FILE = os.path.join(EDITED_FOLDER, "scrap_log.log")
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(LOG_FILE, encoding="utf-8"),
        logging.StreamHandler()
    ]
)


def process_latest_file():
    result = merge_new_products()
    products = result['merged_products']

    processed = []
    batch_size = 20  # Ù‡Ø± Ú†Ù†Ø¯ Ù…Ø­ØµÙˆÙ„ ÛŒÚ©Ø¨Ø§Ø± Ø°Ø®ÛŒØ±Ù‡ Ú©Ù†

    for idx, product in enumerate(products, start=1):
        if product.get("product_link"):

            # Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ù¾ÛŒØ´â€ŒÙØ±Ø¶
            product.setdefault("specifications", [])
            product.setdefault("tags", [])
            product.setdefault("images", [])

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø´Ø®ØµØ§Øª
            if not product.get("checked_specs", False):
                try:
                    new_specs = extract_specifications(product["product_link"])
                    product["specifications"].extend(new_specs)
                    product["checked_specs"] = True
                    logging.info(f"âœ… Ù…Ø´Ø®ØµØ§Øª Ù…Ø­ØµÙˆÙ„ '{product.get('name')}' Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯.")
                except Exception as e:
                    logging.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø´Ø®ØµØ§Øª Ù…Ø­ØµÙˆÙ„ {product.get('name')}: {e}")

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÚ¯â€ŒÙ‡Ø§
            if not product.get("checked_tags", False):
                try:
                    new_tags = extract_tags(product["product_link"])
                    product["tags"].extend([t for t in new_tags if t not in product["tags"]])
                    product["checked_tags"] = True
                    logging.info(f"âœ… ØªÚ¯â€ŒÙ‡Ø§ÛŒ Ù…Ø­ØµÙˆÙ„ '{product.get('name')}' Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯.")
                except Exception as e:
                    logging.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÚ¯ Ù…Ø­ØµÙˆÙ„ {product.get('name')}: {e}")

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØµØ§ÙˆÛŒØ±
            if not product.get("checked_images", False):
                try:
                    new_images = extract_product_images(product["product_link"])
                    product["images"].extend([img for img in new_images if img not in product["images"]])
                    product["checked_images"] = True
                    logging.info(f"âœ… ØªØµØ§ÙˆÛŒØ± Ù…Ø­ØµÙˆÙ„ '{product.get('name')}' Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯.")
                except Exception as e:
                    logging.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØµØ§ÙˆÛŒØ± Ù…Ø­ØµÙˆÙ„ {product.get('name')}: {e}")

            # Ø¨Ø±Ø±Ø³ÛŒ Ù…ÙˆØ¬ÙˆØ¯ÛŒ
            try:
                quantity = extract_quantity(product["product_link"])
                product["quantity"] = quantity
                logging.info(f"âœ… Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ù…Ø­ØµÙˆÙ„ '{product.get('name')}' Ø¨Ø±Ø±Ø³ÛŒ Ø´Ø¯: {quantity}")
            except Exception as e:
                logging.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø±Ø±Ø³ÛŒ Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ù…Ø­ØµÙˆÙ„ {product.get('name')}: {e}")

            processed.append(product)

            # Ø°Ø®ÛŒØ±Ù‡ Ù…ÙˆÙ‚Øª Ù‡Ø± batch_size Ù…Ø­ØµÙˆÙ„
            if idx % batch_size == 0:
                save_partial(processed)
                logging.info(f"ğŸ’¾ Ø°Ø®ÛŒØ±Ù‡ Ù…ÙˆÙ‚Øª Ø¨Ø¹Ø¯ Ø§Ø² {idx} Ù…Ø­ØµÙˆÙ„")

            sleep(1)

    # Ø°Ø®ÛŒØ±Ù‡ Ù†Ù‡Ø§ÛŒÛŒ
    save_final(processed)
    clear_temp_files()
    return {
        "edited_file": "FINAL",
        "products_count": len(processed)
    }


def save_partial(products, suffix="temp"):
    clear_temp_files()
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    filename = f"{suffix}_{timestamp}.json"
    output_path = os.path.join(EDITED_FOLDER, filename)
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(products, f, ensure_ascii=False, indent=2)
    logging.info(f"ğŸ’¾ ÙØ§ÛŒÙ„ Ù…ÙˆÙ‚Øª Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {filename}")


def save_final(products):
    try:
        os.makedirs(EDITED_FOLDER, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M")
        total_count = len(products)
        filename = f"edited_{timestamp}_{total_count}.json"
        output_path = os.path.join(EDITED_FOLDER, filename)
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(products, f, ensure_ascii=False, indent=2)
        logging.info(f"ğŸ’¾ ÙØ§ÛŒÙ„ Ù†Ù‡Ø§ÛŒÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {filename}")
        return True
    except Exception as e:
        logging.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„ Ù†Ù‡Ø§ÛŒÛŒ: {e}")
        return False


def merge_new_products():
    # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø¢Ø®Ø±ÛŒÙ† ÙØ§ÛŒÙ„ raw
    raw_files = [f for f in os.listdir(RAW_FOLDER) if f.startswith("raw_") and f.endswith(".json")]
    if not raw_files:
        raise FileNotFoundError("Ù‡ÛŒÚ† ÙØ§ÛŒÙ„ Ø®Ø§Ù…ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯.")

    raw_files.sort(reverse=True)
    latest_raw_file = raw_files[0]
    raw_path = os.path.join(RAW_FOLDER, latest_raw_file)

    # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø¢Ø®Ø±ÛŒÙ† ÙØ§ÛŒÙ„ ÙˆÛŒØ±Ø§ÛŒØ´ Ø´Ø¯Ù‡
    edited_files = [f for f in os.listdir(EDITED_FOLDER) if f.endswith(".json")]
    previous_edited = None
    if edited_files:
        def extract_datetime(f):
            parts = f.split("_")
            if len(parts) < 4:
                return datetime.min
            date_part = parts[1] if f.startswith("temp_") else parts[2]
            time_part = parts[2] if f.startswith("temp_") else parts[3].split(".")[0]
            try:
                return datetime.strptime(date_part + time_part, "%Y%m%d%H%M")
            except:
                return datetime.min
        edited_files.sort(key=extract_datetime, reverse=True)
        previous_edited = edited_files[0]

    # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§
    with open(raw_path, "r", encoding="utf-8") as f:
        raw_products = json.load(f)

    prev_products = []
    if previous_edited:
        prev_path = os.path.join(EDITED_FOLDER, previous_edited)
        with open(prev_path, "r", encoding="utf-8") as f:
            prev_products = json.load(f)

    prev_slugs = {p.get("product_link") for p in prev_products if p.get("product_link")}
    new_products = [p for p in raw_products if p.get("product_link") not in prev_slugs]

    merged_products = prev_products + new_products

    logging.info(f"ğŸ†• ØªØ¹Ø¯Ø§Ø¯ Ù…Ø­ØµÙˆÙ„Ø§Øª Ø¬Ø¯ÛŒØ¯: {len(new_products)}")
    logging.info(f"ğŸ“¦ Ù…Ø¬Ù…ÙˆØ¹ Ù…Ø­ØµÙˆÙ„Ø§Øª Ù¾Ø³ Ø§Ø² Ø§Ø¯ØºØ§Ù…: {len(merged_products)}")

    return {
        "raw_file": latest_raw_file,
        "previous_edited_file": previous_edited,
        "new_products_count": len(new_products),
        "merged_products": merged_products
    }


def clear_temp_files():
    """Ø­Ø°Ù ØªÙ…Ø§Ù… ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆÙ‚Øª (temp_) Ø§Ø² Ù¾ÙˆØ´Ù‡ EDITED_FOLDER"""
    for f in os.listdir(EDITED_FOLDER):
        if f.startswith("temp_") and f.endswith(".json"):
            temp_path = os.path.join(EDITED_FOLDER, f)
            if os.path.isfile(temp_path):
                try:
                    os.remove(temp_path)
                    logging.info(f"ğŸ—‘ï¸ Ø­Ø°Ù ÙØ§ÛŒÙ„ Ù…ÙˆÙ‚Øª: {f}")
                except Exception as e:
                    logging.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø­Ø°Ù ÙØ§ÛŒÙ„ Ù…ÙˆÙ‚Øª {f}: {e}")


if __name__ == "__main__":
    if os.path.exists(LOCK_FILE):
        logging.warning("âš ï¸ Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ù‚Ø¨Ù„Ø§ Ø¯Ø± Ø­Ø§Ù„ Ø§Ø¬Ø±Ø§Ø³Øª.")
        exit()
    with open(LOCK_FILE, "w") as f:
        f.write("running")
    try:
        process_latest_file()
    finally:
        if os.path.exists(LOCK_FILE):
            os.remove(LOCK_FILE)
